{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8. Question Answeting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIg5l-2Avq8q"
   },
   "source": [
    "# Loading Dataset\n",
    "\n",
    "\n",
    "Question\n",
    "1. What Type of Dataset in Question Answering ?\n",
    "2. How to Solve ? Reading Comprehension\n",
    "3. Agendas : Reading Comprehension Task\n",
    "4. Agendas : Fine-Tuning Pre-Trained Model + Prefix Tuning + Prompt Tuning\n",
    "4. Comparing Different Approach on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bosRvxdnxS8n"
   },
   "source": [
    "Steps :    \n",
    "\n",
    "0. Task Brief\n",
    "1. Loading + Cleaning Fine-Tuning Dataset\n",
    "2. Model Architecture Change\n",
    "3. Fine - Tuning Strategies\n",
    "4. Calculate Metrics based on SQuAD evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WaLs8KbyeRD"
   },
   "source": [
    "# Task Brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9FA3X6UyhSZ"
   },
   "source": [
    "## Dataset : SQuAD Dataset\n",
    "\n",
    "\n",
    "there are lot variety of dataset that we can use to fine tune on Question Answering task,\n",
    "\n",
    "you can browse [here](https://huggingface.co/datasets?task_categories=task_categories:question-answering&sort=trending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Paplg3VRy_-F"
   },
   "source": [
    "Here is a sample from SQuAD Dataset\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Context\n",
    "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXJY9qxmz1Uh"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "question\n",
    "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGNSOPRhz8pb"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "answers\n",
    "{'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUe1lpTO00ou"
   },
   "source": [
    "Each Training Pairs contains :     \n",
    "- Context / Passage\n",
    "- Question\n",
    "- Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aq0dYLLC1dks"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/learning/natural-language-processing/.venv/lib/python3.10/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/ubuntu/learning/natural-language-processing/.venv/lib/python3.10/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Load this library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torchtext import datasets\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt4e_wEC1Ze4"
   },
   "source": [
    "# Loading & Cleaning Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvdKTeTX4CM1"
   },
   "source": [
    "## Loading SQuAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rtcRlLqywpv1",
    "outputId": "2c365392-bebc-4e87-e8c1-2e8169b72a0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/learning/natural-language-processing/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "squad_train = load_dataset(\"squad\", split=\"train[:800]\")\n",
    "\n",
    "squad_valtest= load_dataset(\"squad\", split=\"validation[:200]\")\n",
    "# split the remaining for test\n",
    "squad_valtest = squad_valtest.train_test_split(test_size=0.5)\n",
    "\n",
    "squad_val = squad_valtest['train']\n",
    "squad_test = squad_valtest['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "jjx5LXTXzaqS",
    "outputId": "3c60d3c9-1e6f-41c0-c02c-13ca5aa3d32c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train['context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "siC95FkIzgh7",
    "outputId": "e6f9aa05-215e-45aa-b477-5302d50ba4f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pz6XeLzYziIq",
    "outputId": "7b6cfe2e-8254-426a-e7ca-5a47f9d9f649"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train['answers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmxB5zKT4WBe",
    "outputId": "f72148f7-ec28-4e1c-a2f0-63a3fb0c62ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TsBw96dg4Fnh",
    "outputId": "b64aca44-882b-4e54-f9cf-6f850bd0e680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train SQuAD size (800, 5) | Val SQuAD size (100, 5)| Test SQuAD size (100, 5)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train SQuAD size {squad_train.shape} \"\n",
    "    f\"| Val SQuAD size {squad_val.shape}\"\n",
    "      f\"| Test SQuAD size {squad_test.shape}\"\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3stqCu4w4hBz"
   },
   "source": [
    "That's quite bigh enough dataset, for now we will scale it only by using , 10% from both training and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okwH1YMp4A_h"
   },
   "source": [
    "So in order to use the dataset for training, what input spesification should look like ?\n",
    "\n",
    "\n",
    "- In language model usually we have context --> however in this case we have two information --> `Passage,Questions` , hence we need to combine both as `input`\n",
    "\n",
    "- After that we perform tokenization on input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gf5Ga2NX7TTQ"
   },
   "source": [
    "### Processing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JcOxCzH87x04"
   },
   "outputs": [],
   "source": [
    "example = squad_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aBZJh5BusE1q",
    "outputId": "4ab226d7-b276-4c2f-d561-0733b12768f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-DvCBy2r78hB",
    "outputId": "cc1b99d4-9393-42a6-cd7f-2ea1e72f0838"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LArJqBxSng0Y"
   },
   "source": [
    "### Concatenanting Passage + Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "5XAQi57T8Ybn",
    "outputId": "8bba9b09-c6ef-4b6e-cbb7-d81f6b446104"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = example['context'] + example['question']\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1gaSTJU9Eaw",
    "outputId": "9caabe27-1677-4e8d-f5ef-67e2dcf5f98e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-HY_oSl8wia"
   },
   "source": [
    "Okay, let's see how much maximum length of our input in squad dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Skrx80tL9aFN"
   },
   "source": [
    "Look's like we have maximum input exceed our model context length, GPT2 Context length is `1024`, what we need to do exactly ? Truncate it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvTPK9jQnnAS"
   },
   "source": [
    "### 1. Tokenizing Concatenated Passage + Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "keCMcpTUB8TM"
   },
   "outputs": [],
   "source": [
    "input = tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1sTsEpAm3Og"
   },
   "source": [
    "Let's see what tokenized text looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "nrKJiTdjDBP5",
    "outputId": "acd4bf7b-7628-454f-8ab1-758cff42bfa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdensrI2m8pM"
   },
   "source": [
    "Most of the tokenized text, contain `[PAD]` token, we need to mask them during loss calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiW5vA8_9gSU"
   },
   "source": [
    "How the answer should look like, since we want to perform supervised learning --> we need to provide the model what is the correct answer,\n",
    "\n",
    "Back to our task again , is classifying whether each token is `<start>` or `<end>` of answer ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csA-MwipDII5"
   },
   "source": [
    "### 2. Collect Start & End Position of each Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Tbsbz3nD14a"
   },
   "source": [
    "Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "eoFEfFi2_Agb"
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "\n",
    "        answer = answers[i]\n",
    "\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids()\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "24354d9f1d5340f7aa3c5ce56004fc62",
      "c860714ae019483ca7e88ba3b4852543",
      "546e10a4ce7c4143978707d218409f3c",
      "ebe149594b0d473a9e6af2ea2b6b5ea3",
      "7ec7e742da834c83a52f531506198c30",
      "1b08778af2594905bef526b158ed281d",
      "ec76b836d7bb4d1b9f10617313a5de5e",
      "e35daee8948940b4aefa189224b4531a",
      "3c35eaede6554d5cbbd0221e9010d538",
      "07231c2e7b504e5182e94b04955fc880",
      "2d5fec53534a42f0a50c52e2567844ba",
      "cd886885b5a9464cb5397cf393443e89",
      "13bcbd52543a4435b55692cc9524305d",
      "7f989d90e59d47999a2a4af5860b1b05",
      "cfe1a063bf984d79963d72b1612d3d80",
      "9aba28e8ef264164a23d4465508ce2f3",
      "5b98457308ce4cbf957b598cb5258dfe",
      "447c28b9ec0646b69fae9774bd97688a",
      "649b22ab999347c188e48ec39991ef0b",
      "e85637a0ec6444fa8d8c658b99eee6b1",
      "9bb3000cb246465d8f2488c72cec75e4",
      "0e50748825f24d53b6fd198d1a547d48"
     ]
    },
    "id": "0tx8_I6Az1Ty",
    "outputId": "4188bbc1-323d-424e-a4b8-8289486a584f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 2710.92 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 3101.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "squad_train_tokenized = squad_train.map(preprocess_function, batched=True, remove_columns=['id', 'title','context','question', 'answers']).with_format(\"torch\")\n",
    "squad_val_tokenized = squad_val.map(preprocess_function, batched=True, remove_columns=['id', 'title','context','question', 'answers']).with_format(\"torch\")\n",
    "squad_test_tokenized = squad_test.map(preprocess_function, batched=True, remove_columns=['id', 'title','context','question', 'answers']).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhj7B4H5IMch",
    "outputId": "eb62f408-eba8-4176-cccf-d95db9103462"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "    num_rows: 800\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jtdpJ2ouH5EU"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 8\n",
    "train_loader = DataLoader(squad_train_tokenized,batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(squad_val_tokenized,batch_size=BATCH_SIZE)\n",
    "test_laoder = DataLoader(squad_test_tokenized,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWA3M5kqIPp6",
    "outputId": "08973742-43c3-4f1a-c31a-1bd34d371c5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1\n",
      "tensor([[ 2514,  4150,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   318,   287,  ..., 50256, 50256, 50256],\n",
      "        [  464, 32520,  3970,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2215,   750,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,  1690,   318,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   318,   262,  ..., 50256, 50256, 50256]])\n",
      "Iter 2\n",
      "tensor([[ 2437,   867,  3710,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [ 8496,   318,   262,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,  3925,  2107,  ..., 50256, 50256, 50256],\n",
      "        [13828, 11596,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867, 24218,  ..., 50256, 50256, 50256]])\n",
      "Iter 3\n",
      "tensor([[  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [ 8421,   262,  6282,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867, 13346,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2437,   867, 15273,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373,  2727,  ..., 50256, 50256, 50256],\n",
      "        [13828,  4009,  6875,  ..., 50256, 50256, 50256]])\n",
      "Iter 4\n",
      "tensor([[  464, 21787,   286,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  2099,   286,  ..., 50256, 50256, 50256],\n",
      "        [13828,  1430,   379,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061, 24224,   379,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   318,   262,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256]])\n",
      "Iter 5\n",
      "tensor([[ 2514,  4150,   373,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  1664,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  3923,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [13828,  6802,  2727,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   318,   257,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867, 15619,  ..., 50256, 50256, 50256]])\n",
      "Iter 6\n",
      "tensor([[ 2061,  5873,   286,  ..., 50256, 50256, 50256],\n",
      "        [ 8496,   857, 23382,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  5873,   286,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 1890, 12636,    13,  ..., 50256, 50256, 50256],\n",
      "        [  464, 43485,  1524,  ..., 50256, 50256, 50256],\n",
      "        [  818,  1946,   644,  ..., 50256, 50256, 50256]])\n",
      "Iter 7\n",
      "tensor([[ 2061,  5873,   286,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  1048,   373,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,  2992, 11094,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [13828,  6240,  1908,  ..., 50256, 50256, 50256]])\n",
      "Iter 8\n",
      "tensor([[  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [13828,  1981,  3111,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   750,   262,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [  464, 32684,   917,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750,  2050,  ..., 50256, 50256, 50256]])\n",
      "Iter 9\n",
      "tensor([[   38,   333,   666,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 5886,   703,   867,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [  464, 25043, 10332,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   857,   262,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256]])\n",
      "Iter 10\n",
      "tensor([[ 2061,  2372,   857,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,   739,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  5873,   286,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2437,   867,  2444,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  5873,   286,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  3710,  ..., 50256, 50256, 50256]])\n",
      "Iter 11\n",
      "tensor([[ 2437,   867, 18586,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  2033,   286,  ..., 50256, 50256, 50256],\n",
      "        [ 1858,   389,   703,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2437,  1690,   318,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867, 28022,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  2033,   286,  ..., 50256, 50256, 50256]])\n",
      "Iter 12\n",
      "tensor([[ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,  2099,   286,  ..., 50256, 50256, 50256],\n",
      "        [13828,  4152,   750,  ..., 50256, 50256, 50256],\n",
      "        [  818, 30992,   257,  ..., 50256, 50256, 50256]])\n",
      "Iter 13\n",
      "tensor([[ 5886,   703,   867,  ..., 50256, 50256, 50256],\n",
      "        [ 9627,   508,  9141,  ..., 50256, 50256, 50256],\n",
      "        [  464, 23382, 20377,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2437,   867,   812,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  2260,  ..., 50256, 50256, 50256],\n",
      "        [48919,   661,  5174,  ..., 50256, 50256, 50256]])\n",
      "Iter 14\n",
      "tensor([[ 3673,   260, 20377,  ..., 50256, 50256, 50256],\n",
      "        [13828,  4152,  1893,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  2099,   286,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 8241,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [23820,  7114,  2275,  ..., 50256, 50256, 50256],\n",
      "        [13828,   614,   373,  ..., 50256, 50256, 50256]])\n",
      "Iter 15\n",
      "tensor([[ 1890,   348,   418,  ..., 50256, 50256, 50256],\n",
      "        [24472,   262,   640,  ..., 50256, 50256, 50256],\n",
      "        [13828, 24224,  7411,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [30815,   286,   281,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [  818,   262,   640,  ..., 50256, 50256, 50256]])\n",
      "Iter 16\n",
      "tensor([[ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 7191,   644,   812,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867, 12829,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,  3670,   750,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [ 3152,   644, 24224,  ..., 50256, 50256, 50256]])\n",
      "Iter 17\n",
      "tensor([[ 7191,   644,   812,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2215, 16618,   726,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2215,   750,  1757,  ..., 50256, 50256, 50256],\n",
      "        [  818,  2846,   286,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   373,   262,  ..., 50256, 50256, 50256]])\n",
      "Iter 18\n",
      "tensor([[13828, 13478,   373,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   881,  1637,  ..., 50256, 50256, 50256],\n",
      "        [13828, 28862,   318,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [13828,  1981, 13055,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [13828,  1048, 40394,  ..., 50256, 50256, 50256]])\n",
      "Iter 19\n",
      "tensor([[  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [ 3260,   543,  1981,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,  1588,   287,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [13828,  5888,   373,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  3835,  ..., 50256, 50256, 50256],\n",
      "        [36687,   329,   543,  ..., 50256, 50256, 50256]])\n",
      "Iter 20\n",
      "tensor([[13828,  9283, 10308,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  5873,   286,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [ 2953,   543,  4067,  ..., 50256, 50256, 50256],\n",
      "        [ 3673,   260, 20377,  ..., 50256, 50256, 50256]])\n",
      "Iter 21\n",
      "tensor([[  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373, 23382,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2437,   867, 23715,  ..., 50256, 50256, 50256],\n",
      "        [13828,  1893,   379,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256]])\n",
      "Iter 22\n",
      "tensor([[ 2437,   867,   812,  ..., 50256, 50256, 50256],\n",
      "        [13828,  6899,   379,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867, 43485,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,  4129,   318,  ..., 50256, 50256, 50256],\n",
      "        [  818,   543,  4067,  ..., 50256, 50256, 50256],\n",
      "        [13828, 23566, 11596,  ..., 50256, 50256, 50256]])\n",
      "Iter 23\n",
      "tensor([[  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [ 8496,   318,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  3835,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 4863,   810,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  2214,   286,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   750, 24261,  ..., 50256, 50256, 50256]])\n",
      "Iter 24\n",
      "tensor([[ 2061,   373, 21798,  ..., 50256, 50256, 50256],\n",
      "        [  464,  1524,  1900,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  2099,   286,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2437,  1588,   318,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  4073, 23382,  ..., 50256, 50256, 50256],\n",
      "        [13828,  1160,   400,  ..., 50256, 50256, 50256]])\n",
      "Iter 25\n",
      "tensor([[13828, 15177,  8112,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  2444,  ..., 50256, 50256, 50256],\n",
      "        [ 1858,   547,  3294,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [13828, 11596,   857,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  6253,  ..., 50256, 50256, 50256],\n",
      "        [13828,  1242, 13257,  ..., 50256, 50256, 50256]])\n",
      "Iter 26\n",
      "tensor([[  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [13828,  1981,  4438,  ..., 50256, 50256, 50256],\n",
      "        [13828,  4928,   373,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [13828,  1981,  2540,  ..., 50256, 50256, 50256]])\n",
      "Iter 27\n",
      "tensor([[  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [13828,  5581,  4429,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [13828,  5230,  5243,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   881,   318,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   373,  2323,  ..., 50256, 50256, 50256]])\n",
      "Iter 28\n",
      "tensor([[ 8241,   318,   262,  ..., 50256, 50256, 50256],\n",
      "        [13828,  9312,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 1858,   547, 27278,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,   618,  4495,  ..., 50256, 50256, 50256],\n",
      "        [ 1858,   318,   257,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,  4495,  ..., 50256, 50256, 50256]])\n",
      "Iter 29\n",
      "tensor([[ 2514,   644,  4495,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  3466,  ..., 50256, 50256, 50256],\n",
      "        [ 8496,   750,   262,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 8241,  3058,  3769,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   318,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750,   262,  ..., 50256, 50256, 50256]])\n",
      "Iter 30\n",
      "tensor([[ 2061,   318, 12411,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   318,   262,  ..., 50256, 50256, 50256],\n",
      "        [13828,  1074,   750,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [39276,   543,  1074,  ..., 50256, 50256, 50256],\n",
      "        [  818,  2846,   286,  ..., 50256, 50256, 50256],\n",
      "        [ 2061, 12411,  4346,  ..., 50256, 50256, 50256]])\n",
      "Iter 31\n",
      "tensor([[39276,   543,  6125,  ..., 50256, 50256, 50256],\n",
      "        [13828,  1048, 19152,  ..., 50256, 50256, 50256],\n",
      "        [23672,  1940, 12823,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,  8833,   379,  ..., 50256, 50256, 50256],\n",
      "        [ 4863,   810,   857,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  7259,   857,  ..., 50256, 50256, 50256]])\n",
      "Iter 32\n",
      "tensor([[ 2202,   644,  1110,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  7864,  ...,  8685,  1074,  1201],\n",
      "        [ 2437,   867,  4266,  ...,   262,   749,   416],\n",
      "        ...,\n",
      "        [ 8241,   373,   262,  ...,  1074,  1201, 40417],\n",
      "        [ 8241,  2630,   262,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256]])\n",
      "Iter 33\n",
      "tensor([[ 8241,   318,  4497,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [ 2514,   810,   389,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 8241, 31636,   355,  ..., 50256, 50256, 50256],\n",
      "        [13828,  1048,   373,  ..., 50256, 50256, 50256],\n",
      "        [12130,   440,     6,  ..., 50256, 50256, 50256]])\n",
      "Iter 34\n",
      "tensor([[13828,  4986,   286,  ..., 50256, 50256, 50256],\n",
      "        [13828, 23382, 20377,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   318,   262,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2215,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  3006,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750, 37361,  ..., 50256, 50256, 50256]])\n",
      "Iter 35\n",
      "tensor([[  818,   644,  1748,  ..., 50256, 50256, 50256],\n",
      "        [  818,   543,  5707,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,   371,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [  818,   644,  1748,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2202,   644,  3128,  ..., 50256, 50256, 50256]])\n",
      "Iter 36\n",
      "tensor([[ 2061,   318, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  2597,   750,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2437,   867, 42235,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256]])\n",
      "Iter 37\n",
      "tensor([[ 2215,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 3260,   607,  1218,  ..., 50256, 50256, 50256],\n",
      "        [13828,  6802,   750,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2215,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [13828,  5062,   373,  ..., 50256, 50256, 50256],\n",
      "        [ 3260,   644,  3807,  ..., 50256, 50256, 50256]])\n",
      "Iter 38\n",
      "tensor([[ 2215,   750, 17886,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373, 37361,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [  818,   607,  2647,  ..., 50256, 50256, 50256],\n",
      "        [ 7575,  7093,  3706,  ..., 50256, 50256, 50256],\n",
      "        [13828,  7093,  6875,  ..., 50256, 50256, 50256]])\n",
      "Iter 39\n",
      "tensor([[  818,   543,  5707,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  7093, 13178,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2437,   867,  4406,  ..., 50256, 50256, 50256],\n",
      "        [ 3260,  4305, 17886,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867, 42235,  ..., 50256, 50256, 50256]])\n",
      "Iter 40\n",
      "tensor([[ 2061,  7093,  3706,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,   338,  ..., 50256, 50256, 50256],\n",
      "        [ 8496,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [21993, 27078,   338,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,   338,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  7099,  6621,  ..., 50256, 50256, 50256]])\n",
      "Iter 41\n",
      "tensor([[21993, 27078,   318,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  1664,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [21993,   261, 32682,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  3240,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   373,   262,  ..., 50256, 50256, 50256]])\n",
      "Iter 42\n",
      "tensor([[21993, 27078,  3888,  ..., 50256, 50256, 50256],\n",
      "        [13828,   286,   607,  ..., 50256, 50256, 50256],\n",
      "        [   40,   543,  4928,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,  1748,   373,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,  1468,   373,  ..., 50256, 50256, 50256]])\n",
      "Iter 43\n",
      "tensor([[ 2061, 42644,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,  3066,   284,  ..., 50256, 50256, 50256],\n",
      "        [  818,  8735,    11,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  1588,  1700,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  1700,  1664,  ..., 50256, 50256, 50256]])\n",
      "Iter 44\n",
      "tensor([[ 2953,   644,  2479,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,  1468,   373,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 8241,  4488,   262,  ..., 50256, 50256, 50256],\n",
      "        [13828,  2646,  8096,  ..., 50256, 50256, 50256],\n",
      "        [ 1890,   543,  3496,  ..., 50256, 50256, 50256]])\n",
      "Iter 45\n",
      "tensor([[ 8241,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,   338,  ..., 50256, 50256, 50256],\n",
      "        [  464,  1438, 17886,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061, 14015,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 8496,   750, 17886,  ..., 50256, 50256, 50256],\n",
      "        [24159,  3541,   338,  ..., 50256, 50256, 50256]])\n",
      "Iter 46\n",
      "tensor([[ 2061,   373, 17886,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750, 17886,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 8241,  4855, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  1785,  4073,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   890,   373,  ..., 50256, 50256, 50256]])\n",
      "Iter 47\n",
      "tensor([[ 8241,  4193, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,  6928, 14546,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   373, 13772,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [    1, 37136,   338,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  2745,  ..., 50256, 50256, 50256],\n",
      "        [ 1890,   644,  3127,  ..., 50256, 50256, 50256]])\n",
      "Iter 48\n",
      "tensor([[14574,  2368,  5062,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  4141, 26777,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  5062,  4073,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,  5717,   257,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750, 17886,  ..., 50256, 50256, 50256]])\n",
      "Iter 49\n",
      "tensor([[ 8241,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [13828,  1115,  2678,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078, 31636,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,  2646,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  1588,  2033,  ..., 50256, 50256, 50256],\n",
      "        [ 2061, 12121,   286,  ..., 50256, 50256, 50256]])\n",
      "Iter 50\n",
      "tensor([[ 2061,  3496,   373,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   750,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  2646,   750,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061, 10530, 10997,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  3496,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256]])\n",
      "Iter 51\n",
      "tensor([[21993, 27078,   338,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,   468,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,   338,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061, 12199,  5062,  ..., 50256, 50256, 50256],\n",
      "        [  464,  5062,    11,  ..., 50256, 50256, 50256],\n",
      "        [    1,   464,  1012,  ..., 50256, 50256, 50256]])\n",
      "Iter 52\n",
      "tensor([[13828,  6802,   373,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   318,   262,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [24159,  3541,   338,  ..., 50256, 50256, 50256],\n",
      "        [24159,  3541,   338,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256]])\n",
      "Iter 53\n",
      "tensor([[ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 1026,   373,  3414,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750, 17886,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,  1688,  1785,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   318,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867, 16788,  ..., 50256, 50256, 50256]])\n",
      "Iter 54\n",
      "tensor([[  464,  1085,  2060,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  2678,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867, 25287,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2437,  1029,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   318,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  9088,  ..., 50256, 50256, 50256]])\n",
      "Iter 55\n",
      "tensor([[ 8241, 38261,   351,  ..., 50256, 50256, 50256],\n",
      "        [13828,  2060,   422,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  3807,   750,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [21993, 27078,   338,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   881,  1637,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  5242,  ..., 50256, 50256, 50256]])\n",
      "Iter 56\n",
      "tensor([[ 2061,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   373, 37361,  ..., 50256, 50256, 50256],\n",
      "        [13828,  2646,   750,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  1461, 14015,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,  1392,  ..., 10542,   290, 37361]])\n",
      "Iter 57\n",
      "tensor([[ 9360,  2368,  5062,  ...,  1600,  2957,   284],\n",
      "        [ 1890,   543,  5707,  ...,  1600,  2957,   284],\n",
      "        [13828, 14015,  4405,  ...,   278,   262, 10542],\n",
      "        ...,\n",
      "        [ 2215,   750, 37361,  ...,   262, 10542,   290],\n",
      "        [ 8241,  4405,   503,  ...,   262, 10542,   290],\n",
      "        [ 2437,   881,   750,  ...,   278,   262, 10542]])\n",
      "Iter 58\n",
      "tensor([[ 2215,   750, 37361,  ..., 37361, 32682, 37080],\n",
      "        [ 8241,   750, 37361,  ..., 32682, 37080,  1710],\n",
      "        [ 8241,   318, 37361,  ...,   290, 37361, 32682],\n",
      "        ...,\n",
      "        [21993, 27078, 19152,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,  2921,  ..., 50256, 50256, 50256],\n",
      "        [13828,  3496,   750,  ..., 50256, 50256, 50256]])\n",
      "Iter 59\n",
      "tensor([[ 2061, 12121,   286,  ..., 50256, 50256, 50256],\n",
      "        [   32,  1907,  3715,  ..., 50256, 50256, 50256],\n",
      "        [ 8496,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2437,   881,   517,  ..., 50256, 50256, 50256],\n",
      "        [13828, 14015,   750,  ..., 50256, 50256, 50256],\n",
      "        [13828,  4009,  2722,  ..., 50256, 50256, 50256]])\n",
      "Iter 60\n",
      "tensor([[ 8496,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [13828, 32251,  2646,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867, 13304,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2437,   867,  1271,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,  8165,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,  2722,  ..., 50256, 50256, 50256]])\n",
      "Iter 61\n",
      "tensor([[ 2061,  3496,   373,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,  2073,  4120,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   750,   484,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 8241,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [21993,   261, 32682,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   750, 37361,  ..., 50256, 50256, 50256]])\n",
      "Iter 62\n",
      "tensor([[21993, 27078,   561,  ..., 50256, 50256, 50256],\n",
      "        [13828,   614,   750,  ..., 50256, 50256, 50256],\n",
      "        [13828,  5863, 20533,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2215,   750,   673,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   890,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   750, 37361,  ..., 50256, 50256, 50256]])\n",
      "Iter 63\n",
      "tensor([[ 8241,  5220,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   890,   750,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [21993, 27078,  2627,  ..., 50256, 50256, 50256],\n",
      "        [13828,  4009,   750,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,   373,  ..., 50256, 50256, 50256]])\n",
      "Iter 64\n",
      "tensor([[   39,  2577,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750,   428,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   750,   673,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 8241,  2716,   262,  ..., 50256, 50256, 50256],\n",
      "        [13828,  4009,   750,  ..., 50256, 50256, 50256],\n",
      "        [21993,   261, 32682,  ..., 50256, 50256, 50256]])\n",
      "Iter 65\n",
      "tensor([[21993, 27078,   338,  ..., 50256, 50256, 50256],\n",
      "        [13828,  2060,   550,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,  1839,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2437,   867,  9088,  ..., 50256, 50256, 50256],\n",
      "        [ 8241, 11343, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750,   673,  ..., 50256, 50256, 50256]])\n",
      "Iter 66\n",
      "tensor([[ 2061,   318,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   373,   604,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  9088,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2215,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 8496,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,   338,  ..., 50256, 50256, 50256]])\n",
      "Iter 67\n",
      "tensor([[ 9360,   717,  5585,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2215,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 8496,   373,  4518,  ..., 50256, 50256, 50256]])\n",
      "Iter 68\n",
      "tensor([[ 8496,   373, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867, 12513,  ..., 50256, 50256, 50256],\n",
      "        [24159,  3541,   338,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2215,   750, 17886,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2953,  3025, 23166,  ..., 50256, 50256, 50256]])\n",
      "Iter 69\n",
      "tensor([[ 2437,   867, 24205,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750, 17886,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,   318,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  9667,  ..., 50256, 50256, 50256],\n",
      "        [ 3198,   286, 37361,  ..., 50256, 50256, 50256]])\n",
      "Iter 70\n",
      "tensor([[21993, 27078,  2630,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078, 21346,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750,   262,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,  3496,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  9667,  ..., 50256, 50256, 50256]])\n",
      "Iter 71\n",
      "tensor([[13828, 14235, 20447,  ..., 50256, 50256, 50256],\n",
      "        [21993,   261, 32682,  ..., 50256, 50256, 50256],\n",
      "        [13828,  2095,   287,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 8496,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,  5399, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,  2098, 37361,  ..., 50256, 50256, 50256]])\n",
      "Iter 72\n",
      "tensor([[ 2437,   881,   517,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  4875,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2437,   867, 13304,  ..., 50256, 50256, 50256],\n",
      "        [13828,  6802,  4405,  ..., 50256, 50256, 50256],\n",
      "        [13828,  7093,   750,  ..., 50256, 50256, 50256]])\n",
      "Iter 73\n",
      "tensor([[21993, 27078,   561,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,  1718,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,  2626,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2202,   644,  7093,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   561,   673,  ..., 50256, 50256, 50256],\n",
      "        [ 3152,   644,  3517,  ..., 50256, 50256, 50256]])\n",
      "Iter 74\n",
      "tensor([[ 2437,   867, 13304,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867, 13304,  ..., 50256, 50256, 50256],\n",
      "        [21993,   261, 32682,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [21993, 27078,  2716,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,   338,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  1110,   750,  ..., 50256, 50256, 50256]])\n",
      "Iter 75\n",
      "tensor([[ 2437,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  1611,   286,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 1722,   286,  3035,  ..., 50256, 50256, 50256],\n",
      "        [ 8496,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,  3417,  ..., 50256, 50256, 50256]])\n",
      "Iter 76\n",
      "tensor([[ 8241,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   547, 37361,  ..., 50256, 50256, 50256],\n",
      "        [41631,   703,  4406,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [  818,   543,  2647,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   547, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867,  4406,  ..., 50256, 50256, 50256]])\n",
      "Iter 77\n",
      "tensor([[ 2061,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 8496,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [30568,  1168,   290,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 8496,   750,   673,  ..., 50256, 50256, 50256],\n",
      "        [ 5195,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   772,   373,  ..., 50256, 50256, 50256]])\n",
      "Iter 78\n",
      "tensor([[ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  3496,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 8496,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [30568,  1168,   468,  ..., 50256, 50256, 50256],\n",
      "        [13828,  3496,   416,  ..., 50256, 50256, 50256]])\n",
      "Iter 79\n",
      "tensor([[ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2953,   644,  4436,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2215,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  3496,   750,  ..., 50256, 50256, 50256]])\n",
      "Iter 80\n",
      "tensor([[ 2061,   318,  3017,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   373,  4518,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,   290,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,   750,   673,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   750,   484,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   750,   673,  ..., 50256, 50256, 50256]])\n",
      "Iter 81\n",
      "tensor([[ 2061,  3496,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  3496,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  1919,  2056,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [21993, 27078,  4855,  ..., 50256, 50256, 50256],\n",
      "        [ 8496,   373, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750,   673,  ..., 50256, 50256, 50256]])\n",
      "Iter 82\n",
      "tensor([[ 2061,  1923,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  3496,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   857, 10274,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [21993, 27078,  4488,  ..., 50256, 50256, 50256],\n",
      "        [ 2025,  1593,  4725,  ..., 50256, 50256, 50256],\n",
      "        [  464,  3850, 37361,  ..., 50256, 50256, 50256]])\n",
      "Iter 83\n",
      "tensor([[ 2061,   550,   284,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2514,  4150,   373,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 8241,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   373,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373, 17799,  ..., 50256, 50256, 50256]])\n",
      "Iter 84\n",
      "tensor([[ 2061,   750,   262,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,  1863,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,   351,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [21993, 27078,  4405,  ...,  3690,  2795,  2211],\n",
      "        [21993, 27078,   290,  ...,  2211,   784,  2795],\n",
      "        [21993, 27078,  2627,  ...,  2795,  1946,    13]])\n",
      "Iter 85\n",
      "tensor([[ 4933,  1566,  1737,  ...,  2211,   784,  2795],\n",
      "        [25262,  3648,   290,  ...,  2211,   784,  2795],\n",
      "        [  818,  2321,   508,  ...,  2795,  1946,    13],\n",
      "        ...,\n",
      "        [ 2061,   318, 37361,  ...,   770,   287,  1210],\n",
      "        [ 8241,  2540,  6447,  ...,  2795,  1946,    13],\n",
      "        [ 2215,   750, 37361,  ...,  2795,  1946,    13]])\n",
      "Iter 86\n",
      "tensor([[ 8241, 11001,   326,  ...,   784,  2795,  1946],\n",
      "        [ 2215,   750,  9180,  ...,  1510,  3690,  2795],\n",
      "        [21993, 27078,   338,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [13828,  6980,   373,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867, 19318,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   750,   262,  ..., 50256, 50256, 50256]])\n",
      "Iter 87\n",
      "tensor([[ 2061,   857, 15564,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   466,   584,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   867, 19318,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,   857,   449,  ..., 50256, 50256, 50256],\n",
      "        [22648,   422, 37361,  ..., 50256, 50256, 50256],\n",
      "        [23937,   371,     5,  ..., 50256, 50256, 50256]])\n",
      "Iter 88\n",
      "tensor([[21993, 27078,  4632,  ..., 50256, 50256, 50256],\n",
      "        [43584,  7259, 37361,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,   373,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,   584,  3303,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  5062,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  3918,   286,  ..., 50256, 50256, 50256]])\n",
      "Iter 89\n",
      "tensor([[ 2061,  3303,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 8241, 31764, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  5062,   750,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,  7505,   373,  ..., 50256, 50256, 50256],\n",
      "        [ 3152,  9180,  1168,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   857,   673,  ..., 50256, 50256, 50256]])\n",
      "Iter 90\n",
      "tensor([[ 2061,   636,   286,  ..., 50256, 50256, 50256],\n",
      "        [21993,   261, 32682,  ..., 50256, 50256, 50256],\n",
      "        [  818,  3090,   284,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [21993, 27078,  2722,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,   468,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,  5399,  ..., 50256, 50256, 50256]])\n",
      "Iter 91\n",
      "tensor([[ 2215,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  1295,   318,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,  5610,   607,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 8241,  2921, 37361,  ..., 50256, 50256, 50256],\n",
      "        [21993,   261, 32682,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  1271,   373,  ..., 50256, 50256, 50256]])\n",
      "Iter 92\n",
      "tensor([[ 2514,  4150,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,  1468,   373,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,  2921,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,  3496,   416,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   318, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   373, 37361,  ..., 50256, 50256, 50256]])\n",
      "Iter 93\n",
      "tensor([[ 8241,   857, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   857,   673,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  3496,  4073,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 8241, 13338, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  3807, 12824,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   750,   412,  ..., 50256, 50256, 50256]])\n",
      "Iter 94\n",
      "tensor([[ 8496,   750,   673,  ..., 50256, 50256, 50256],\n",
      "        [ 2061, 14015,  7867,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  3496,   750,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [21993, 27078,   468,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,  7620, 16717,  ..., 50256, 50256, 50256],\n",
      "        [   39,  2577,   857,  ..., 50256, 50256, 50256]])\n",
      "Iter 95\n",
      "tensor([[ 2437,   857,   673,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   857, 42345,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   857,   673,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 8241,   857, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,  7867, 37361,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,   550,  ..., 50256, 50256, 50256]])\n",
      "Iter 96\n",
      "tensor([[21993, 27078,   550,  ..., 50256, 50256, 50256],\n",
      "        [  464,   337, 17485,  ..., 50256, 50256, 50256],\n",
      "        [  464,   337, 17485,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 8496,   750, 13691,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  4097,  6971,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   318,   262,  ..., 50256, 50256, 50256]])\n",
      "Iter 97\n",
      "tensor([[   50, 30302, 40084,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   389, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750,   383,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 1890,   644,   857,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,  7690,   607,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   468,   406,  ..., 50256, 50256, 50256]])\n",
      "Iter 98\n",
      "tensor([[ 2437,   857, 14862,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   468, 37361,  ..., 50256, 50256, 50256],\n",
      "        [ 8241,   468,   531,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,   614,   750,  ..., 50256, 50256, 50256],\n",
      "        [21993, 27078,  3181,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   857,   673,  ..., 50256, 50256, 50256]])\n",
      "Iter 99\n",
      "tensor([[18602,   644,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   318,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2215,   750, 37361,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [21993, 27078,   338,  ..., 50256, 50256, 50256],\n",
      "        [13828,  2647, 10099,  ..., 50256, 50256, 50256],\n",
      "        [13828,  1573, 29013,  ..., 50256, 50256, 50256]])\n",
      "Iter 100\n",
      "tensor([[36476,    88,   677,  ..., 50256, 50256, 50256],\n",
      "        [38208,  3841, 28261,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  1573,   318,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 8128,   286, 37361,  ..., 50256, 50256, 50256],\n",
      "        [  818,   644,   614,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,   857, 37361,  ..., 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "# check sampling\n",
    "\n",
    "for idx,x in enumerate(train_loader) :\n",
    "  print(f'Iter {idx+1}')\n",
    "  print(x['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zO2G0q81_Jw"
   },
   "source": [
    "# Model Architecture Change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtbiWMbB2CBe"
   },
   "source": [
    "## Load Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCTMBOSo2E4N"
   },
   "source": [
    "To perform fine-tuning process we usually have a base model / pre-trained model which learned from these objectives :     \n",
    "- Mask Language Modelling / Next Word Prediction / Span Corruption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlBx00Wt2nyw"
   },
   "source": [
    "We load these components :    \n",
    "1. Pretrained Model\n",
    "2. Trained Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "binlctKHPzFH"
   },
   "source": [
    "### Hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QTh-A_IUw1Va"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "gpt = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Lbk8NFam-2A9"
   },
   "outputs": [],
   "source": [
    "class DistilledGPT2ForQA(nn.Module) :\n",
    "    def __init__(self,pretrained_model,device) :\n",
    "        super().__init__()\n",
    "        # take last layer / before logit\n",
    "        self.pretrained_model = pretrained_model.transformer\n",
    "        # take pretrained_model embedding dimension\n",
    "        self.n_dim  = 768\n",
    "        # qa_outputs out_features --> 2 , the label is start and end span\n",
    "        self.n_outputs = 2\n",
    "        self.qa_outputs = nn.Linear(self.n_dim,self.n_outputs)\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def forward(self,input_ids= None,\n",
    "        attention_mask= None,\n",
    "        token_type_ids= None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions= None,\n",
    "        output_hidden_states= None,\n",
    "        return_dict= None) :\n",
    "      # feed to pretrained first\n",
    "      B,T = input_ids.shape\n",
    "      position_ids = torch.arange(0, T, dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "      x = self.pretrained_model(input_ids=input_ids,attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids,position_ids=position_ids,\n",
    "                                head_mask=head_mask,inputs_embeds=inputs_embeds,\n",
    "                                output_attentions=output_attentions,\n",
    "                                output_hidden_states=output_hidden_states,return_dict=return_dict)\n",
    "      print(type(x))\n",
    "\n",
    "      # get logit\n",
    "      logit = self.qa_outputs(x)\n",
    "\n",
    "      # return logit\n",
    "      return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTkWnAAeMxv_",
    "outputId": "d1567bde-7bb8-46e2-ae4c-51ed2bda1e84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'offset_mapping'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blqoEaMKP2DP"
   },
   "source": [
    "### Appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jkXHaORg-Rh5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ahqRSuQP4lJ",
    "outputId": "6988b0d5-ac5c-44f6-f72f-1222c09e77a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model_qa = AutoModelForQuestionAnswering.from_pretrained('distilgpt2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmPyIxI6-l3r",
    "outputId": "8bb6c95c-02a9-425c-80fd-e43dd4783f01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForQuestionAnswering(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9-d-CfisPdUD"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XmXMstQjBQ-U",
    "outputId": "c6cd3f2c-81b7-44fb-a089-7a7e4fe04a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 2514,  4150,   750,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   318,   287,  ..., 50256, 50256, 50256],\n",
      "        [  464, 32520,  3970,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2215,   750,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 2437,  1690,   318,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   318,   262,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'start_positions': tensor([132,   0,   0,   0,   0,  64,  98, 125]), 'end_positions': tensor([139,   0,   0,   0,   0,  66,  98, 126])}\n"
     ]
    }
   ],
   "source": [
    "for train_samples in train_loader :\n",
    "    print(train_samples)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w6HlGOgXJXUY",
    "outputId": "264b0bac-cec5-4c10-c2ea-a0997e85a8f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 384])\n"
     ]
    }
   ],
   "source": [
    "for train_samples in train_loader :\n",
    "  input_ids = train_samples['input_ids'].to(device)\n",
    "  print(input_ids.shape)\n",
    "  # attention mask\n",
    "  attention_mask = train_samples['attention_mask'].to(device)\n",
    "\n",
    "  # load model\n",
    "  logit = model_qa(input_ids=input_ids,attention_mask=attention_mask)\n",
    "\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "up9fwnG9A1A8"
   },
   "source": [
    "Extract the logit score (last layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "gdaV7nol-a-G"
   },
   "outputs": [],
   "source": [
    "start_logit = logit.start_logits\n",
    "end_logit = logit.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "PYdKi5k--uNP"
   },
   "outputs": [],
   "source": [
    "start_softmax = torch.nn.functional.softmax(start_logit,dim=-1)\n",
    "end_softmax = torch.nn.functional.softmax(end_logit,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "fSVOK0Y0_29S"
   },
   "outputs": [],
   "source": [
    "start_ans = torch.topk(start_softmax,k=1).indices\n",
    "end_ans = torch.topk(end_softmax,k=1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "_arQxrMSALgO",
    "outputId": "4c930913-c346-47fe-a188-52d72b36ff42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(start_ans[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "F1hfL22cASLJ",
    "outputId": "fcff2e37-6067-4881-d7a6-76a73ffe1764"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(end_ans[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "UYf8cuwiDDjo",
    "outputId": "3b7862d6-355d-4308-c7d6-fa1e64a44778"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train['context'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEYZ3Xq5GAvJ"
   },
   "source": [
    "Try one sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXOKznEoiBn4"
   },
   "source": [
    "### Retrain the model + Parameter Efficient Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-c_56s4iiEFQ"
   },
   "source": [
    "but before that we should create training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pq6eC-KnC1Qq",
    "outputId": "67d82c2f-5b21-472d-d8e6-c51daa95c755"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.5.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.47.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.2)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.27.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "U15xSE2tw_PQ",
    "outputId": "a0a186fe-3a80-4b39-8550-eb1393df67a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXOFqONJCuYK"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "hyP18XHMw-FR"
   },
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "model_qa.config.pad_token_id = model_qa.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "68LqAL47tU0z",
    "outputId": "70757053-b980-41c6-de5b-01a305d447fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/learning/natural-language-processing/.venv/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/ubuntu/learning/natural-language-processing/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/tmp/ipykernel_5082/2345955528.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='147' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [147/250 24:51 < 17:39, 0.10 it/s, Epoch 2.92/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.178417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.542708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"finetuned_distil_gpt2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_qa,\n",
    "    args=training_args,\n",
    "    train_dataset=squad_train_tokenized,\n",
    "    eval_dataset=squad_val_tokenized,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHNHyE37yIbK"
   },
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "3SY8FZOpyJqW"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"content/finetuned_distil_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "3OF4sedEx9lG"
   },
   "outputs": [],
   "source": [
    "model_finetuned =AutoModelForQuestionAnswering.from_pretrained('content/finetuned_distil_gpt2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5pLx1NAs2uSe",
    "outputId": "dd446870-17cc-4bcd-a507-b4c711e339d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForQuestionAnswering(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_finetuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2Lgx9_O2uj2"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "_O7dg9L849th"
   },
   "outputs": [],
   "source": [
    "example = squad_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rdXvEeZU6Syy",
    "outputId": "26055ee8-940f-41bd-c2c3-629736f4b586"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56bf21b43aeaaa14008c9526',\n",
       " 'title': 'Super_Bowl_50',\n",
       " 'context': \"The league announced on October 16, 2012, that the two finalists were Sun Life Stadium and Levi's Stadium. The South Florida/Miami area has previously hosted the event 10 times (tied for most with New Orleans), with the most recent one being Super Bowl XLIV in 2010. The San Francisco Bay Area last hosted in 1985 (Super Bowl XIX), held at Stanford Stadium in Stanford, California, won by the home team 49ers. The Miami bid depended on whether the stadium underwent renovations. However, on May 3, 2013, the Florida legislature refused to approve the funding plan to pay for the renovations, dealing a significant blow to Miami's chances.\",\n",
       " 'question': 'What was the last Super Bowl that took place at Sun Life Stadium in Miami? ',\n",
       " 'answers': {'text': ['Super Bowl XLIV', 'Super Bowl XLIV', '2010'],\n",
       "  'answer_start': [242, 242, 261]}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "ArN9LnjY41xn"
   },
   "outputs": [],
   "source": [
    "\n",
    "inputs = tokenizer.encode_plus(example['question'],example['context'],return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EykyhoOv5Jhs",
    "outputId": "bc9bab12-ab1c-45b6-8e99-e5692b3c47f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 149])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "Gk2LFZa45QKL"
   },
   "outputs": [],
   "source": [
    "outputs = model_finetuned(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "hLPQuFI_7WPm"
   },
   "outputs": [],
   "source": [
    "answer_start = torch.argmax(outputs[0])\n",
    "answer_end = torch.argmax(outputs[1]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rrRaSwvS5jdn",
    "outputId": "97c81043-7e94-4da5-be35-ac3ccc07d314"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 149])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logit = outputs.start_logits\n",
    "start_logit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4cs-3TE5qMg",
    "outputId": "5d7f11fd-1224-432d-b8e9-b8ea82fb073c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.0493,  -8.7085,  -8.1685,  -6.5768,  -7.2861,  -6.9952,  -8.5725,\n",
       "          -6.7387, -10.6892,  -9.1734,  -8.0847,  -5.4625,  -3.7948,  -7.7111,\n",
       "          -4.8642,  -7.6763,  -7.0149,  -8.7619,  -7.2607,  -9.7084,  -8.2131,\n",
       "          -6.5272,  -5.7186,  -9.4201,  -3.6045,  -9.1746, -10.5124,  -8.9089,\n",
       "          -6.4716,  -6.3749,  -9.6435,  -7.9720,  -5.8928,  -5.1505,  -9.8025,\n",
       "          -5.4918,  -6.7729,  -5.2714,  -6.0076,  -7.5251,  -7.0350,  -7.5446,\n",
       "          -9.4085,  -7.5396,  -6.6433,  -8.0638,  -8.1219,  -8.8850,  -8.6233,\n",
       "          -5.8670,  -4.4821,  -4.4662,  -9.1933,  -9.0341, -10.2257, -10.6135,\n",
       "          -8.0383, -11.3468,  -7.3806,  -5.6575,  -8.7674, -11.1200,  -9.4360,\n",
       "          -7.4665,  -9.4113,  -9.3786, -10.4345,  -8.1279,  -8.4324,  -6.4169,\n",
       "          -3.5528,  -8.8779,  -2.5434,  -6.1163,  -7.5707,  -6.2057,  -6.8929,\n",
       "          -7.7318,  -4.8213,  -7.2300,  -8.8359,  -8.4083,  -2.1599,  -9.4574,\n",
       "          -8.0993,  -7.5200,  -6.5956,  -4.3735,  -7.2223,  -8.6096,  -7.5314,\n",
       "          -3.1104,  -2.7165,  -7.9632,  -4.5318,  -7.6602,  -6.1723,  -8.7489,\n",
       "          -8.1445,  -9.7682,  -9.3491,  -8.4448,  -6.5956,  -6.2868,  -3.2488,\n",
       "          -6.0191,  -7.1553,  -6.8753,  -7.3605,  -5.2661,  -9.6157,  -6.4599,\n",
       "          -7.6042,  -6.9786,  -8.8566,  -4.9963,  -5.9592,  -5.5994,  -7.6366,\n",
       "          -8.2683,  -5.1822,  -4.3398,  -8.6712,  -3.2518,  -8.2119,  -8.6569,\n",
       "          -8.3470,  -4.4438,  -5.6272,  -8.4511,  -6.6831,  -8.4456,  -5.9241,\n",
       "          -5.1068, -10.1140,  -7.7512,  -7.8525,  -8.1635,  -5.2778,  -9.1185,\n",
       "          -5.5395,  -7.3455,  -5.8107,  -3.8362,  -8.2151,  -4.7207,  -8.3617,\n",
       "          -6.1806,  -6.3923]], device='cuda:0', grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_logit = outputs.end_logits\n",
    "end_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-sE0QwmjHRdt",
    "outputId": "c3f9bae0-c150-4aed-ddd4-e5df9de73a52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 149])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_logit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5uSJ3rq51WC",
    "outputId": "e3d937ca-72df-421e-e88b-d4df285438f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract position\n",
    "answer_start = torch.topk(F.softmax(start_logit,dim=-1),k=1).indices.item()\n",
    "answer_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPZuIRZT5_2o",
    "outputId": "83aaca36-81de-481f-dc9f-e27eb074fd82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract position\n",
    "answer_end = (torch.topk(F.softmax(end_logit,dim=-1),k=1).indices).item()\n",
    "answer_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_qlz4dAH9W4",
    "outputId": "58943001-98b5-4a33-93e3-5833e5d404ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2061, 373]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_token = inputs['input_ids'][:,answer_start:answer_end+1].view(-1).tolist()\n",
    "ans_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "7PK6eh7n6JNd",
    "outputId": "d39bb2c2-b32f-4156-8a13-171466ff1e26"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'What was'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = tokenizer.decode(ans_token)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rBvDC7_YHfnM",
    "outputId": "66d5da73-2934-475f-835d-d215efafdc2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2061]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "QVGXkS693dOD"
   },
   "outputs": [],
   "source": [
    "def generate_answer(context, question,model=model_finetuned,tokenizer=tokenizer):\n",
    "  inputs = tokenizer.encode_plus(context,question,return_tensors='pt',\n",
    "                                 truncation=\"only_second\",return_offsets_mapping=False,\n",
    "                                 padding=\"max_length\").to(device)\n",
    "  outputs = model(**inputs)\n",
    "  start_logit = outputs.start_logits\n",
    "  end_logit = outputs.end_logits\n",
    "  answer_start = torch.topk(F.softmax(start_logit,dim=-1),k=1).indices.item()\n",
    "  answer_end = torch.topk(F.softmax(end_logit,dim=-1),k=1).indices.item()\n",
    "  ans_token = inputs['input_ids'][:,answer_start:answer_end+1].view(-1).tolist()\n",
    "  answer  = tokenizer.decode(ans_token)\n",
    "\n",
    "  return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKgo94YT8NW1",
    "outputId": "6f137150-af4e-4671-d09e-dabc5f949de2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_test_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4va_cNNE7-Wp",
    "outputId": "7be1d96a-240e-4667-e8af-7f3607df4873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "For\n",
      "The\n",
      "The\n",
      "The\n",
      "The\n",
      "The\n",
      "In\n",
      "Super\n",
      "Super\n"
     ]
    }
   ],
   "source": [
    "for i in range(10) :\n",
    "  data = squad_test[i]\n",
    "  context = data['context']\n",
    "  question = data['question']\n",
    "  ans = generate_answer(question=question,context=context,model=model_finetuned,tokenizer=tokenizer)\n",
    "  print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bZIyLdqItmE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07231c2e7b504e5182e94b04955fc880": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e50748825f24d53b6fd198d1a547d48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "13bcbd52543a4435b55692cc9524305d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b98457308ce4cbf957b598cb5258dfe",
      "placeholder": "​",
      "style": "IPY_MODEL_447c28b9ec0646b69fae9774bd97688a",
      "value": "Map: 100%"
     }
    },
    "1b08778af2594905bef526b158ed281d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24354d9f1d5340f7aa3c5ce56004fc62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c860714ae019483ca7e88ba3b4852543",
       "IPY_MODEL_546e10a4ce7c4143978707d218409f3c",
       "IPY_MODEL_ebe149594b0d473a9e6af2ea2b6b5ea3"
      ],
      "layout": "IPY_MODEL_7ec7e742da834c83a52f531506198c30"
     }
    },
    "2d5fec53534a42f0a50c52e2567844ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c35eaede6554d5cbbd0221e9010d538": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "447c28b9ec0646b69fae9774bd97688a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "546e10a4ce7c4143978707d218409f3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e35daee8948940b4aefa189224b4531a",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c35eaede6554d5cbbd0221e9010d538",
      "value": 100
     }
    },
    "5b98457308ce4cbf957b598cb5258dfe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "649b22ab999347c188e48ec39991ef0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ec7e742da834c83a52f531506198c30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f989d90e59d47999a2a4af5860b1b05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_649b22ab999347c188e48ec39991ef0b",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e85637a0ec6444fa8d8c658b99eee6b1",
      "value": 100
     }
    },
    "9aba28e8ef264164a23d4465508ce2f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9bb3000cb246465d8f2488c72cec75e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c860714ae019483ca7e88ba3b4852543": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b08778af2594905bef526b158ed281d",
      "placeholder": "​",
      "style": "IPY_MODEL_ec76b836d7bb4d1b9f10617313a5de5e",
      "value": "Map: 100%"
     }
    },
    "cd886885b5a9464cb5397cf393443e89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_13bcbd52543a4435b55692cc9524305d",
       "IPY_MODEL_7f989d90e59d47999a2a4af5860b1b05",
       "IPY_MODEL_cfe1a063bf984d79963d72b1612d3d80"
      ],
      "layout": "IPY_MODEL_9aba28e8ef264164a23d4465508ce2f3"
     }
    },
    "cfe1a063bf984d79963d72b1612d3d80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9bb3000cb246465d8f2488c72cec75e4",
      "placeholder": "​",
      "style": "IPY_MODEL_0e50748825f24d53b6fd198d1a547d48",
      "value": " 100/100 [00:00&lt;00:00, 655.85 examples/s]"
     }
    },
    "e35daee8948940b4aefa189224b4531a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e85637a0ec6444fa8d8c658b99eee6b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ebe149594b0d473a9e6af2ea2b6b5ea3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07231c2e7b504e5182e94b04955fc880",
      "placeholder": "​",
      "style": "IPY_MODEL_2d5fec53534a42f0a50c52e2567844ba",
      "value": " 100/100 [00:00&lt;00:00, 389.01 examples/s]"
     }
    },
    "ec76b836d7bb4d1b9f10617313a5de5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
